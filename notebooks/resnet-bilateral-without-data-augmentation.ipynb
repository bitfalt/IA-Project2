{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.transforms import Resize, InterpolationMode\n",
    "from torchsummary import summary\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "from torchaudio.transforms import MelSpectrogram, FrequencyMasking, TimeMasking\n",
    "import librosa\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from torchmetrics import ConfusionMatrix\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectogram(specgram, title=None, ylabel='freq_bin'):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin='lower', aspect='auto')\n",
    "\n",
    "def plot_history(history, net_name):\n",
    "    x_ticks = range(1, len(history['train']['loss']) + 1)\n",
    "    for item in history['train'].keys():\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for prefix, color in zip(['train', 'val'], ['r', 'b']):\n",
    "            plt.plot(x_ticks, history[prefix][item], c=color, alpha=0.75, linestyle='--',\n",
    "                     label=prefix)\n",
    "        plt.title('{} {}'.format(net_name, item))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xticks(x_ticks)\n",
    "        plt.ylabel(item)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def plot_metric_values(metric_values, net_name):\n",
    "    for item in metric_values:\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        x = range(metric_values[item].shape[0])\n",
    "        mean = metric_values[item].cpu().mean()\n",
    "        bars = ax.bar(x, metric_values[item].cpu().numpy())\n",
    "        \n",
    "        ax.axhline(mean, color='r', linestyle=':')\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            \n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, height, '{:.2}'.format(height),\n",
    "                    ha='center', va='bottom')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_title('{} {} on test'.format(net_name, item))\n",
    "        plt.show()\n",
    "\n",
    "def generate_confusion_matrix(net, dataloader, device, num_classes, idx_to_class,\n",
    "                              title=\"Confusion Matrix\", save_path=None, log_to_wandb=True):\n",
    "    \"\"\"\n",
    "    Generates and plots a confusion matrix for the given network and dataloader.\n",
    "\n",
    "    Args:\n",
    "        net: The trained PyTorch model.\n",
    "        dataloader: The PyTorch DataLoader for the dataset (e.g., test_dl).\n",
    "        device: The device ('cuda' or 'cpu') the model and data are on.\n",
    "        num_classes: The total number of classes.\n",
    "        idx_to_class: A dictionary mapping integer class indices to string class names.\n",
    "                      (e.g., {0: 'class_a', 1: 'class_b', ...}).\n",
    "        title: The title for the confusion matrix plot.\n",
    "        save_path: Optional path to save the plot image (e.g., 'confusion_matrix.png').\n",
    "        log_to_wandb: Whether to log the confusion matrix to Weights & Biases.\n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    confmat = ConfusionMatrix(task='multiclass', num_classes=num_classes).to(device)\n",
    "\n",
    "    print(\"Generating confusion matrix...\")\n",
    "\n",
    "    # Iterate through the dataloader and update the confusion matrix metric\n",
    "    with torch.no_grad():\n",
    "        # Using tqdm again to show progress during confusion matrix generation\n",
    "        for X, y in tqdm(dataloader, desc=\"Calculating Confusion Matrix\"):\n",
    "            if X is None or y is None:\n",
    "                 continue\n",
    "\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = net(X)\n",
    "            confmat.update(preds, y)\n",
    "\n",
    "    confusion_matrix_tensor = confmat.compute()\n",
    "\n",
    "    confusion_matrix_np = confusion_matrix_tensor.cpu().numpy()\n",
    "\n",
    "    class_names = [idx_to_class[i] for i in range(num_classes)]\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Use ConfusionMatrixDisplay for easy plotting\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_np, display_labels=class_names)\n",
    "\n",
    "    # Customize the plot\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d')\n",
    "    ax.set_title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if save_path is provided\n",
    "    if save_path:\n",
    "        try:\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            print(f\"Confusion matrix plot saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving confusion matrix plot to {save_path}: {e}\")\n",
    "\n",
    "\n",
    "    # Log the plot to Weights & Biases if log_to_wandb is True\n",
    "    if log_to_wandb:\n",
    "        try:\n",
    "            wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "            print(\"Confusion matrix plot logged to W&B.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging confusion matrix to W&B: {e}\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadedSpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load processed spectrogram tensors saved in\n",
    "    a directory structure like:\n",
    "    base_dir/\n",
    "    ├── class_label_1/\n",
    "    │   ├── file1.pt\n",
    "    │   ├── file2.pt\n",
    "    │   └── ...\n",
    "    ├── class_label_2/\n",
    "    │   ├── fileA.pt\n",
    "    │   └── ...\n",
    "    └── ...\n",
    "    \"\"\"\n",
    "    def __init__(self, data_base_dir: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_base_dir: The path to the base directory containing the class subdirectories\n",
    "                           with saved .pt files (e.g., 'RawFreqMasking').\n",
    "        \"\"\"\n",
    "        self.data_base_dir = data_base_dir\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        self._load_files()\n",
    "\n",
    "    def _load_files(self):\n",
    "        \"\"\"Scans the data_base_dir to find all .pt files and their class labels.\"\"\"\n",
    "        print(f\"Scanning directory: {os.path.abspath(self.data_base_dir)}\")\n",
    "\n",
    "        # Get a list of potential class directories (first level subdirectories)\n",
    "        class_dirs = [d for d in os.listdir(self.data_base_dir)\n",
    "                      if os.path.isdir(os.path.join(self.data_base_dir, d))]\n",
    "\n",
    "        # Sort class directories to ensure consistent class_to_idx mapping\n",
    "        class_dirs.sort()\n",
    "\n",
    "        # Create class_to_idx and idx_to_class mappings\n",
    "        for idx, class_name in enumerate(class_dirs):\n",
    "            self.class_to_idx[class_name] = idx\n",
    "            self.idx_to_class[idx] = class_name\n",
    "\n",
    "        print(f\"Found {len(class_dirs)} classes: {list(self.class_to_idx.keys())}\")\n",
    "\n",
    "        # Iterate through class directories and find all .pt files\n",
    "        for class_name in tqdm(class_dirs, desc=\"Loading file paths\"):\n",
    "            class_dir_path = os.path.join(self.data_base_dir, class_name)\n",
    "            class_label_int = self.class_to_idx[class_name]\n",
    "\n",
    "            # List all files in the class directory\n",
    "            files_in_class_dir = os.listdir(class_dir_path)\n",
    "\n",
    "            # Filter for .pt files and store their full paths and integer labels\n",
    "            for filename in files_in_class_dir:\n",
    "                if filename.endswith('.pt'):\n",
    "                    file_path = os.path.join(class_dir_path, filename)\n",
    "                    # Store (full_file_path, integer_class_label)\n",
    "                    self.samples.append((file_path, class_label_int))\n",
    "\n",
    "        print(f\"Total files found: {len(self.samples)}\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - specgram: The loaded spectrogram tensor (torch.Tensor).\n",
    "            - class_label: The integer class label (int).\n",
    "        \"\"\"\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        file_path, class_label_int = self.samples[idx]\n",
    "\n",
    "        try:\n",
    "            # Load the tensor from the .pt file\n",
    "            specgram = torch.load(file_path)\n",
    "\n",
    "            return specgram, class_label_int\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {file_path}: {e}\")\n",
    "\n",
    "            # Let's return None and require manual handling in the DataLoader loop\n",
    "            return None, None\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LoadedSpectrogramDataset('../dataset/BilateralWithDataAugmentation')\n",
    "n_classes = len(ds.class_to_idx.keys())\n",
    "print(f\"Loaded dataset contains {len(ds)} samples\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "sample = ds[0]\n",
    "print(f\"Sample shape: {sample[0].shape} class: {sample[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "n_mels = 24\n",
    "time = 48\n",
    "sample_rate = 48000\n",
    "\n",
    "transform = torch.nn.Sequential(\n",
    "    MelSpectrogram(sample_rate, n_fft=1024, n_mels=n_mels),\n",
    "    Resize(size=(n_mels, time), interpolation=InterpolationMode.NEAREST),\n",
    "    FrequencyMasking(freq_mask_param=15),\n",
    "    TimeMasking(time_mask_param=35),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 118\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(ds, [0.7, 0.15, 0.15])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size)\n",
    "test_dl = DataLoader(test_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B (ResNet 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A basic residual block for ResNet.\n",
    "    Consists of two 3x3 convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            stride: Stride for the first convolutional layer (used for spatial downsampling).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection (identity or 1x1 convolution for downsampling/channel change)\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "\n",
    "        # If stride > 1 or input channels do not match output channels,\n",
    "        # we need a convolutional layer in the shortcut.\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride,\n",
    "                          bias=False),\n",
    "                torch.nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store the original input for the shortcut connection\n",
    "        identity = x\n",
    "\n",
    "        # Pass through the main path (conv -> bn -> relu -> conv -> bn)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Apply the shortcut connection to the original input and add it\n",
    "        out += self.shortcut(identity)\n",
    "\n",
    "        # Apply the final ReLU *after* the addition\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A custom ResNet-like model built from BasicBlocks.\n",
    "    Structure similar to ResNet-18's feature extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, block, num_blocks, num_classes=10, in_channels=1, dropout_prob=0.5):\n",
    "        \"\"\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            block: The block type to use (e.g., BasicBlock).\n",
    "            num_blocks: A list specifying the number of blocks in each stage.\n",
    "                        e.g., [2, 2, 2, 2] for a structure similar to ResNet-18.\n",
    "            num_classes: The number of output classes for the final classifier.\n",
    "            in_channels: The number of input channels (1 for spectrograms).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.num_classes = num_classes\n",
    "        self.base_in_channels = in_channels # Store original input channels (1)\n",
    "\n",
    "        # --- Initial Layers ---\n",
    "        # Adapt the first conv layer to accept 3 input channels after expansion in forward\n",
    "        self.conv1 = torch.nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # --- Residual Stages ---\n",
    "        # Stage 1: No spatial downsampling (stride=1), maintains 64 channels\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        # Stage 2: Spatial downsampling (stride=2), doubles channels to 128\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        # Stage 3: Spatial downsampling (stride=2), doubles channels to 256\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # Stage 4: Spatial downsampling (stride=2), doubles channels to 512\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "\n",
    "        # --- Final Layers ---\n",
    "        # AdaptiveAvgPool2d reduces spatial dimensions to 1x1 regardless of input size\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Final fully connected layer for classification\n",
    "        # Input features are the output channels of the last stage (512) * block expansion (1 for BasicBlock)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob) # To reduce overfitting\n",
    "        self.fc = torch.nn.Linear(512 * (block.expansion if hasattr(block, 'expansion') else 1), num_classes)\n",
    "\n",
    "        # --- Initialize Weights ---\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                # Initialize BN weights to 1 and biases to 0\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                # Initialize linear layer weights and biases\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"\n",
    "        Helper function to create a sequence of residual blocks for a stage.\n",
    "        \"\"\"\n",
    "        # Determine strides for blocks in this layer.\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "\n",
    "        # Iterate through the strides to create blocks\n",
    "        for stride in strides:\n",
    "            # Create a block, updating self.in_channels for the next block\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            # After creating a block, the input channels for the *next* block\n",
    "            # in this layer will be the *output* channels of the block just created.\n",
    "            self.in_channels = out_channels * (block.expansion if hasattr(block, 'expansion') else 1)\n",
    "\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is [batch_size, 1, H, W] from the DataLoader\n",
    "        # Expand the single channel to 3 by repeating\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Pass through Initial Layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Pass through Residual Stages\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "    \n",
    "        # Pass through Final Layers\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1) # Flatten to [batch_size, channels]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x) # Classifier\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo\n",
    "model = CustomResNet(block=BasicBlock, num_blocks=[2, 2, 2, 2], num_classes=n_classes, in_channels=n_channels)\n",
    "model = model.to(device)\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "summary(model, (n_channels, n_mels, time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    entity=\"bitfalt-itcr\",\n",
    "    project=\"resnet-bilateral-without-data-augmentation-hyperparameter-tuning\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    "    tags=[\"Resnet\", \"MNIST Audio\", \"Bilateral\", \"Without Data Augmentation\"],\n",
    "    notes=\"Run to find optimal hyperparameters\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_dl, val_dl, n_epochs, criterion, metrics, device, lr):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    history = {\n",
    "        'train': {'loss': []} | {metric: [] for metric in metrics},\n",
    "        'val': {'loss': []} | {metric: [] for metric in metrics}\n",
    "    }\n",
    "    \n",
    "    net.to(device)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = torch.tensor(0.0).to(device)\n",
    "        train_metric_values = {\n",
    "            metric: torch.tensor(0.0).to(device) for metric in metrics\n",
    "        }\n",
    "        \n",
    "        net.train()\n",
    "        for X, y in tqdm(train_dl, desc='Epoch {}/{}'.format(epoch + 1, n_epochs),\n",
    "                         total=len(train_dl)):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            preds = net(X)\n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_loss += loss.detach().cpu() * train_dl.batch_size\n",
    "                for metric in metrics:\n",
    "                    train_metric_values[metric] += train_dl.batch_size * \\\n",
    "                        metrics[metric](preds, y)\n",
    "        \n",
    "        train_loss /= len(train_dl.dataset)\n",
    "        for metric in metrics:\n",
    "            train_metric_values[metric] /= len(train_dl.dataset)\n",
    "        \n",
    "        print('train_loss: {:.3f}'.format(train_loss), end=', ')\n",
    "        for metric in metrics:\n",
    "            print('train_{}: {:.3f}'.format(metric, train_metric_values[metric]),\n",
    "                  end=', ')\n",
    "\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Train Loss\": train_loss,\n",
    "                \"Train \" + metric: train_metric_values[metric],\n",
    "            })\n",
    "        \n",
    "        history['train']['loss'].append(train_loss.cpu().detach().numpy().item())\n",
    "        for metric in metrics:\n",
    "            history['train'][metric].append(train_metric_values[metric].cpu().detach() \\\n",
    "                                            .numpy().item())\n",
    "            \n",
    "        val_loss, val_metric_values = eval(net, val_dl, prefix='val', criterion=criterion,\n",
    "                                           metrics=metrics, device=device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['val']['loss'].append(val_loss.cpu().detach().numpy().item())\n",
    "        for metric in metrics:\n",
    "            history['val'][metric].append(val_metric_values[metric].cpu().detach() \\\n",
    "                                          .numpy().item())\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Val Loss\": val_loss,\n",
    "                \"Val \" + metric: val_metric_values[metric],\n",
    "            })\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, eval_dl, prefix, criterion, metrics, device):\n",
    "    eval_loss = torch.tensor(0.0).to(device)\n",
    "    eval_metric_values = {\n",
    "        metric: torch.zeros(metrics[metric].num_classes if metrics[metric].average is None\n",
    "                            else 1).to(device) for metric in metrics\n",
    "    }\n",
    "    \n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in eval_dl:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            preds = net(X)\n",
    "            loss = criterion(preds, y)\n",
    "            \n",
    "            eval_loss += loss.detach().cpu() * eval_dl.batch_size\n",
    "            for metric in metrics:\n",
    "                eval_metric_values[metric] += metrics[metric](preds, y) * \\\n",
    "                    eval_dl.batch_size\n",
    "    \n",
    "    eval_loss /= len(eval_dl.dataset)\n",
    "    for metric in metrics:\n",
    "        eval_metric_values[metric] /= len(eval_dl.dataset)\n",
    "    \n",
    "    print('{}_loss: {:.3f}'.format(prefix, eval_loss), end='')\n",
    "    for metric in metrics:\n",
    "        print(', {}_{}: {:.3f}'.format(prefix, metric, eval_metric_values[metric].mean()),\n",
    "              end='')\n",
    "        \n",
    "        if prefix == 'test':\n",
    "            lossKey = f\"{prefix} Loss\"\n",
    "            metricKey = f\"{prefix} {metric}\"\n",
    "            wandb.log({\n",
    "                lossKey: eval_loss,\n",
    "                metricKey: eval_metric_values[metric].mean(),\n",
    "            })\n",
    "    \n",
    "    return eval_loss, eval_metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_metrics = {\n",
    "    'accuracy': Accuracy(task='multiclass', num_classes=int(n_classes),\n",
    "                         average='macro').to(device),\n",
    "    'precision': Precision(task='multiclass', num_classes=int(n_classes),\n",
    "                           average='macro').to(device),\n",
    "    'recall': Recall(task='multiclass', num_classes=int(n_classes),\n",
    "                     average='macro').to(device),\n",
    "    'f1-score': F1Score(task='multiclass', num_classes=int(n_classes),\n",
    "                        average='macro').to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(model, train_dl, val_dl, n_epochs=10, criterion=criterion,\n",
    "                metrics=train_metrics, device=device, lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, net_name='ResNet Custom')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\n",
    "    'accuracy': Accuracy(task='multiclass', num_classes=int(n_classes),\n",
    "                         average=None).to(device),\n",
    "    'precision': Precision(task='multiclass', num_classes=int(n_classes),\n",
    "                           average=None).to(device),\n",
    "    'recall': Recall(task='multiclass', num_classes=int(n_classes),\n",
    "                     average=None).to(device),\n",
    "    'f1-score': F1Score(task='multiclass', num_classes=int(n_classes),\n",
    "                        average=None).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_metric_values = eval(model, test_dl, prefix='test', criterion=criterion,\n",
    "                             metrics=test_metrics, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'loaded_dataset' in locals() and hasattr(loaded_dataset, 'idx_to_class'):\n",
    "   idx_to_class_mapping = loaded_dataset.idx_to_class\n",
    "elif 'test_dataset' in locals() and hasattr(test_dataset, 'idx_to_class'):\n",
    "   idx_to_class_mapping = test_dataset.idx_to_class\n",
    "else:\n",
    "   print(\"Warning: idx_to_class mapping not found. Using integer labels for confusion matrix.\")\n",
    "   idx_to_class_mapping = {i: str(i) for i in range(int(n_classes))}\n",
    "\n",
    "\n",
    "# Call the function to generate the confusion matrix plot\n",
    "generate_confusion_matrix(\n",
    "    net=model,\n",
    "    dataloader=test_dl,\n",
    "    device=device,\n",
    "    num_classes=int(n_classes),\n",
    "    idx_to_class=idx_to_class_mapping,\n",
    "    title=\"Test Set Confusion Matrix\",\n",
    "    save_path=\"resnet_bilateral_without_data_augmentation_confusion_matrix.png\",\n",
    "    log_to_wandb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_values(test_metric_values, net_name='ResNet Custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
